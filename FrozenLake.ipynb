# Environment Setup
import warnings
import gym
import numpy as np

# Suppress all DeprecationWarnings globally
warnings.simplefilter("ignore", DeprecationWarning)

# Define a custom 8x8 FrozenLake map with specific holes
def create_custom_map():
    return [
        "SFFFFFFF",
        "FFFFFFFF",
        "FFFFHFFF",
        "FFFFFFFF",
        "HHHFFHHH",
        "FFFFFFFF",
        "FFFFHFFF",
        "FFFFFGFF"
    ]

# Initialize FrozenLake environment with custom map
custom_map = create_custom_map()
env = gym.make('FrozenLake-v1', desc=custom_map, is_slippery=True, new_step_api=True)

# --- Markdown Explanation ---
# **Environment Setup:**
# - Import necessary libraries
# - Suppress warnings
# - Define a custom 8x8 grid with a specific hole placement
# - Initialize the FrozenLake environment with this custom map

# Defining Key Variables
n_states = env.observation_space.n  # Total states in the environment
n_actions = env.action_space.n  # Number of possible actions

gamma = 0.9  # Discount factor
theta = 1e-6  # Threshold for value iteration convergence

V = np.zeros(n_states)  # State-value function initialized to zeros
policy = np.zeros(n_states, dtype=int)  # Policy initialized to take action 0 in all states

# --- Markdown Explanation ---
# **Key Variables:**
# - `gamma` (Discount Factor): Determines the importance of future rewards.
# - `theta`: Convergence threshold for Value Iteration.
# - `V`: State-value function, storing expected rewards for each state.
# - `policy`: Stores the best action to take from each state.

# Value Iteration Implementation
def value_iteration():
    global V, policy
    while True:
        delta = 0
        new_V = np.copy(V)
        for s in range(n_states):
            q_values = np.zeros(n_actions)
            for a in range(n_actions):
                for prob, next_state, reward, done in env.unwrapped.P[s][a]:
                    if done:
                        q_values[a] += prob * reward
                    else:
                        q_values[a] += prob * (reward + gamma * V[next_state])
            new_V[s] = max(q_values)
            delta = max(delta, abs(V[s] - new_V[s]))
            policy[s] = np.argmax(q_values)
        V = new_V
        if delta < theta:
            break

# --- Markdown Explanation ---
# **Value Iteration Algorithm:**
# - Iteratively updates the state-value function (`V`).
# - Computes action-values (`q_values`) for each state-action pair.
# - Updates the policy to choose the best action per state.

# --- Markdown Explanation ---
# **Bellman Update in the Code:**
# The Bellman update equation appears in the following lines inside `value_iteration`:
# ```python
# q_values[a] += prob * (reward + gamma * V[next_state])
# ```
# This implements the Bellman equation:
# V(s) = max_a Σ P(s' | s, a) [ R(s, a, s') + γ V(s') ]
# - This equation updates the state-value function (`V`) based on expected rewards and future values.
# - `gamma` discounts future rewards, ensuring convergence.

# Extract Optimal Policy
def extract_policy():
    for s in range(n_states):
        q_values = np.zeros(n_actions)
        for a in range(n_actions):
            for prob, next_state, reward, done in env.unwrapped.P[s][a]:
                if done:
                    q_values[a] += prob * reward
                else:
                    q_values[a] += prob * (reward + gamma * V[next_state])
        policy[s] = np.argmax(q_values)

# Display Policy Function
def display_policy():
    actions = ['\u2190', '\u2193', '\u2192', '\u2191']  # Left, Down, Right, Up
    grid_size = int(np.sqrt(n_states))
    lake_map = env.unwrapped.desc
    print("\nOptimal Policy:\n")
    for i in range(grid_size):
        row = ""
        for j in range(grid_size):
            state = i * grid_size + j
            tile = lake_map[i, j].decode("utf-8")
            if tile == 'H':
                cell = 'H'  # Hole
            elif tile == 'G':
                cell = 'G'  # Goal
            elif tile == 'S':
                cell = 'S'  # Start position
            else:
                cell = actions[policy[state]]
            row += f"| {cell} "
        row += "|"
        print(row)
        print("-" * (grid_size * 4 + 1))

# Execute the functions
value_iteration()
extract_policy()
display_policy()

# --- Markdown Explanation ---
# **Experimentation with Policy Execution:**
# The function `run_experiment` evaluates the learned policy by running multiple episodes in the environment.
# - It follows the optimal policy for `num_episodes` times and records:
#   - Total reward collected in each episode.
#   - Number of steps taken in each episode.
# - It returns the average reward and average number of steps per episode to assess policy performance.
# - Higher rewards indicate better policy performance.

def run_experiment(policy, env, num_episodes=1000):
    rewards = []
    steps = []
    for _ in range(num_episodes):
        obs = env.reset()
        total_reward = 0
        step_count = 0
        done = False
        while not done:
            action = policy[obs]
            obs, reward, terminated, truncated, _ = env.step(action)
            done = terminated or truncated
            total_reward += reward
            step_count += 1
        rewards.append(total_reward)
        steps.append(step_count)

    avg_reward = np.mean(rewards)
    avg_steps = np.mean(steps)
    print(f"\nAfter {num_episodes} episodes:")
    print(f"Average reward: {avg_reward}")
    print(f"Average number of steps per episode: {avg_steps}")
    return avg_reward, avg_steps
